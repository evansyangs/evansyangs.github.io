(window.webpackJsonp=window.webpackJsonp||[]).push([[70],{530:function(t,s,a){"use strict";a.r(s);var n=a(15),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("div",{staticClass:"custom-block tip"},[a("p",{staticClass:"custom-block-title"},[t._v("提示")]),t._v(" "),a("p",[t._v("Use TensorFlow to take Machine Learning to the next level. Your new skills will amaze you.")])]),t._v(" "),a("h2",{attrs:{id:"_1-intro-to-dl-for-computer-vision"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-intro-to-dl-for-computer-vision"}},[t._v("#")]),t._v(" 1. Intro to DL for Computer Vision")]),t._v(" "),a("h3",{attrs:{id:"intro"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("After the end of this lesson, you will understand convolutions. Convolutions are the basic building block for deep learning models in computer vision (and many other applications).")]),t._v(" "),a("p",[t._v("After that, we'll quickly progress to using world-class deep learning models.")]),t._v(" "),a("h3",{attrs:{id:"lesson"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\n\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'wG6rdUURU-w'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/wG6rdUURU-w?start=2" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"exercise-intro-to-dl-for-computer-vision"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-intro-to-dl-for-computer-vision"}},[t._v("#")]),t._v(" Exercise: Intro to DL for Computer Vision")]),t._v(" "),a("p",[t._v("After the video, "),a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/kernels/fork/499266",target:"_blank",rel:"noopener noreferrer"}},[t._v("test your understanding of convolutions"),a("OutboundLink")],1)])]),t._v(" "),a("h4",{attrs:{id:"intro-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-2"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("You don't directly choose the numbers to go into your convolutions for deep learning... instead the deep learning technique determines what convolutions will be useful from the data (as part of model-training). We'll come back to how the model does that soon.")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://i.imgur.com/op9Maqr.png",width:"60%",height:"40%",alt:"convolutions"}})]),t._v(" "),a("p",[t._v("But looking closely at convolutions and how they are applied to your image will improve your intuition for these models, how they work, and how to debug them when they don't work.")]),t._v(" "),a("h4",{attrs:{id:"exercise-1"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-1"}},[t._v("#")]),t._v(" Exercise 1")]),t._v(" "),a("p",[t._v("In the video, you saw a convolution that detected horizontal lines. That convolution shows up again in the code cell below.")]),t._v(" "),a("p",[t._v("Run the cell to see a raw image as well as the output from applying this convolution to the image.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("horizontal_line_conv "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                        "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# load_my_image and visualize_conv are utility functions provided for this exercise")]),t._v("\noriginal_image "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" load_my_image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" \nvisualize_conv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("original_image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" horizontal_line_conv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("p",[t._v("Now it's your turn. Instead of a horizontal line detector, you will create a vertical line detector.")]),t._v(" "),a("p",[a("strong",[t._v("Replace the underscores with numbers to make a vertical line detector and uncomment both lines of code in the cell below. Then run")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("vertical_line_conv "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                      "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nq_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvisualize_conv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("original_image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" vertical_line_conv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h4",{attrs:{id:"exercise-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-2"}},[t._v("#")]),t._v(" Exercise 2")]),t._v(" "),a("p",[t._v("The convolutions you've seen are 2x2.  But you could have larger convolutions. They could be 3x3, 4x4, etc.  They don't even have to be square. Nothing prevents using a 4x7 convolution.")]),t._v(" "),a("p",[t._v("Compare the number of visual patterns that can be captured by small convolutions. Which of the following is true?")]),t._v(" "),a("ul",[a("li",[t._v("There are more visual patterns that can be captured by large convolutions")]),t._v(" "),a("li",[t._v("There are fewer visual patterns that can be captured by large convolutions")]),t._v(" "),a("li",[t._v("The number of visual patterns that can be captured by large convolutions is the same as the number of visual patterns that can be captured by small convolutions?")])]),t._v(" "),a("p",[t._v("Once you think you know the answer, check it by uncommenting and running the line below.")]),t._v(" "),a("p",[a("strong",[t._v("Solution")]),t._v(": While any one convolution measures only a single pattern, there are more possible convolutions that can be created with large sizes. So there are also more patterns that can be captured with large convolutions.")]),t._v(" "),a("p",[t._v("For example, it's possible to create a 3x3 convolution that filters for bright pixels with a dark one in the middle. There is no configuration of a 2x2 convolution that would capture this.")]),t._v(" "),a("p",[t._v("On the other hand, anything that can be captured by a 2x2 convolution could also be captured by a 3x3 convolution.")]),t._v(" "),a("p",[t._v("Does this mean powerful models require extremely large convolutions? Not necessarily. In the next lesson, you will see how deep learning models put together many convolutions to capture complex patterns... including patterns to complex to be captured by any single convolution.")]),t._v(" "),a("h2",{attrs:{id:"_2-building-models-from-convolutions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-building-models-from-convolutions"}},[t._v("#")]),t._v(" 2. Building Models From Convolutions")]),t._v(" "),a("h3",{attrs:{id:"intro-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-3"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("By the end of this lesson, you will understand how convolutions are combined to enable superhuman achievements in computer vision.")]),t._v(" "),a("h3",{attrs:{id:"lesson-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-2"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ToBPiUlLFEY'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/ToBPiUlLFEY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("Now that you understand the structure of the models, "),a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/dansbecker/programming-in-tensorflow-and-keras",target:"_blank",rel:"noopener noreferrer"}},[t._v("learn how to build them"),a("OutboundLink")],1)]),t._v(" with TensorFlow and Keras.")]),t._v(" "),a("h2",{attrs:{id:"_3-tensorflow-programming"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-tensorflow-programming"}},[t._v("#")]),t._v(" 3. TensorFlow Programming")]),t._v(" "),a("h3",{attrs:{id:"intro-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-4"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("At the end of this lesson, you will be able to write TensorFlow and Keras code to use one of the best models in computer vision.")]),t._v(" "),a("h3",{attrs:{id:"lesson-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-3"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sDG5tPtsbSA'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/sDG5tPtsbSA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"sample-code"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-code"}},[t._v("#")]),t._v(" Sample Code")]),t._v(" "),a("h4",{attrs:{id:"choose-images-to-work-with"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#choose-images-to-work-with"}},[t._v("#")]),t._v(" Choose Images to Work With")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" join\n\nimage_dir "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/dog-breed-identification/train/'")]),t._v("\nimg_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" filename "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" \n                           "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0246f44bb123ce3f91c939861eb97fb7.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'84728e78632c0910a69d33f82e62638c.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'8825e914555803f4c67b26593c9d5aff.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                            "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'91a5e8db15bccfb6cfa2df5e8b95ec03.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("h4",{attrs:{id:"function-to-read-and-prep-images-for-modeling"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#function-to-read-and-prep-images-for-modeling"}},[t._v("#")]),t._v(" Function to Read and Prep Images for Modeling")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet50 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" preprocess_input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" load_img"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_to_array\n\nimage_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("read_and_prep_images")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    imgs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("load_img"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_height"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_width"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" img_path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    img_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("img_to_array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" img "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" imgs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    output "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("h4",{attrs:{id:"create-model-with-pre-trained-weights-file-make-predictions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#create-model-with-pre-trained-weights-file-make-predictions"}},[t._v("#")]),t._v(" Create Model with Pre-Trained Weights File. Make Predictions")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ResNet50\n\nmy_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npreds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" my_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h4",{attrs:{id:"visualize-predictions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#visualize-predictions"}},[t._v("#")]),t._v(" Visualize Predictions")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deep_learning"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decode_predictions "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" decode_predictions\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" display\n\nmost_likely_labels "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" decode_predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" class_list_path"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/imagenet_class_index.json'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    display"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("most_likely_labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("h3",{attrs:{id:"exercise-tensorflow-programming"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-tensorflow-programming"}},[t._v("#")]),t._v(" Exercise: TensorFlow Programming")]),t._v(" "),a("p",[t._v("Now you are ready to "),a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/kernels/fork/521452",target:"_blank",rel:"noopener noreferrer"}},[t._v("use a powerful TensorFlow model"),a("OutboundLink")],1)]),t._v(" yourself.")]),t._v(" "),a("h4",{attrs:{id:"intro-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-5"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("The TV show "),a("em",[t._v("Silicon Valley")]),t._v(' had an app called "See Food" that promised to identify food.')]),t._v(" "),a("p",[t._v("In this notebook, you will write code using and comparing pre-trained models to choose one as an engine for the See Food app.")]),t._v(" "),a("p",[t._v("You won't go too deep into Keras or TensorFlow details in this particular exercise.  Don't worry. You'll go deeper into model development soon.  For now, you'll make sure you know how to use pre-trained models.")]),t._v(" "),a("h4",{attrs:{id:"set-up"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#set-up"}},[t._v("#")]),t._v(" Set-Up")]),t._v(" "),a("p",[t._v("We will run a few steps of environmental set-up before writing your own code. "),a("strong",[t._v("You don't need to understand the details of this set-up code.")]),t._v(" You can just run each code cell until you get to the exercises.")]),t._v(" "),a("h5",{attrs:{id:"_1-create-image-paths"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-create-image-paths"}},[t._v("#")]),t._v(" 1) Create Image Paths")]),t._v(" "),a("p",[t._v("This workspace includes image files you will use to test your models. Run the cell below to store a few filepaths to these images in a variable "),a("code",[t._v("img_paths")]),t._v(".")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" os\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" os"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" join\n\n\nhot_dog_image_dir "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/hot-dog-not-hot-dog/seefood/train/hot_dog'")]),t._v("\n\nhot_dog_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hot_dog_image_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" filename "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" \n                            "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1000288.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                             "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'127117.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nnot_hot_dog_image_dir "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/hot-dog-not-hot-dog/seefood/train/not_hot_dog'")]),t._v("\nnot_hot_dog_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("join"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("not_hot_dog_image_dir"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" filename"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" filename "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v("\n                            "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'823536.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                             "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'99890.jpg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n\nimg_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" hot_dog_paths "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" not_hot_dog_paths\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br")])]),a("h5",{attrs:{id:"_2-run-an-example-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-run-an-example-model"}},[t._v("#")]),t._v(" 2) Run an Example Model")]),t._v(" "),a("p",[t._v("Here is the code you saw in the tutorial. It loads data, loads a pre-trained model, and makes predictions. Run this cell too.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" display\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deep_learning"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("decode_predictions "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" decode_predictions\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet50 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" preprocess_input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ResNet50\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" load_img"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_to_array\n\n\nimage_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("read_and_prep_images")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    imgs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("load_img"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_height"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_width"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" img_path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    img_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("img_to_array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" img "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" imgs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    output "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("output"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\nmy_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npreds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" my_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmost_likely_labels "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" decode_predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br")])]),a("h5",{attrs:{id:"_3-visualize-predictions"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-visualize-predictions"}},[t._v("#")]),t._v(" 3) Visualize Predictions")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_path "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("enumerate")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    display"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Image"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("most_likely_labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("You will write a couple useful functions in the next exercises. Then you will put these functions together to compare the effectiveness of various pretrained models for your hot-dog detection program.")]),t._v(" "),a("h4",{attrs:{id:"exercise-1-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-1-2"}},[t._v("#")]),t._v(" Exercise 1")]),t._v(" "),a("p",[t._v("We want to distinguish whether an image is a hot dog or not. But our models classify pictures into 1000 different categories. Write a function that takes the models predictions (in the same format as "),a("code",[t._v("preds")]),t._v(" from the set-up code) and returns a list of "),a("code",[t._v("True")]),t._v(" and "),a("code",[t._v("False")]),t._v(" values.")]),t._v(" "),a("p",[t._v("Some tips:")]),t._v(" "),a("ul",[a("li",[t._v("Work iteratively. Figure out one line at a time outsie the function, and print that line's output to make sure it's right. Once you have all the code you need, move it into the function "),a("code",[t._v("is_hot_dog")]),t._v(". If you get an error, check that you have copied the right code and haven't left anything out.")]),t._v(" "),a("li",[t._v("The raw data we loaded in "),a("code",[t._v("img_paths")]),t._v(" had two images of hot dogs, followed by two images of other foods. So, if you run your function on "),a("code",[t._v("preds")]),t._v(", which represents the output of the model on these images, your function should return "),a("code",[t._v("[True, True, False, False]")]),t._v(".")]),t._v(" "),a("li",[t._v("You will want to use the "),a("code",[t._v("decode_predictions")]),t._v(" function that was also used in the code provided above. We provided a line with this in the code cell to get you started.")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Experiment with code outside the function, then move it into the function once you think it is right")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the following lines are given as a hint to get you started")]),t._v("\ndecoded "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" decode_predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("decoded"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("is_hot_dog")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v("'''\n    inputs:\n    preds_array:  array of predictions from pre-trained model\n\n    outputs:\n    is_hot_dog_list: a list indicating which predictions show hotdog as the most likely label\n    '''")]),t._v("\n    res"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    most_likely_labels "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" decode_predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" most_likely_label "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" most_likely_labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" most_likely_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'hotdog'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            res"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n            res"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" res\n        \nq_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br")])]),a("p",[a("strong",[t._v("Solution")]),t._v(":")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("is_hot_dog")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    decoded "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" decode_predictions"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# pull out predicted label, which is in d[0][1] due to how decode_predictions structures results")]),t._v("\n    labels "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("d"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" d "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" decoded"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    out "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("l "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'hotdog'")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" l "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" labels"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br")])]),a("h4",{attrs:{id:"exercise-2-evaluate-model-accuracy"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-2-evaluate-model-accuracy"}},[t._v("#")]),t._v(" Exercise 2: Evaluate Model Accuracy")]),t._v(" "),a("p",[t._v("You have a model (called "),a("code",[t._v("my_model")]),t._v("). Is it good enough to build your app around?")]),t._v(" "),a("p",[t._v("Find out by writing a function that calculates a model's accuracy (fraction correct). You will try an alternative model in the next step. So we will put this logic in a reusable function that takes data and the model as arguments, and returns the accuracy.")]),t._v(" "),a("p",[t._v("Tips:")]),t._v(" "),a("ul",[a("li",[t._v("Use the "),a("code",[t._v("is_hot_dog")]),t._v(" function from above to help write your function")]),t._v(" "),a("li",[t._v("To save you some scrolling, here is the code from above where we used a TensorFlow model to make predictions:")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("my_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ntest_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\npreds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" my_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("calc_accuracy")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" paths_to_hotdog_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" paths_to_other_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    hotdog_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paths_to_hotdog_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    hotdog_preds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hotdog_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    other_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paths_to_other_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    other_preds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    correct_preds"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hotdog_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    all_preds"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hotdog_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other_preds"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" correct_preds"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("all_preds\n    \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Code to call calc_accuracy.  my_model, hot_dog_paths and not_hot_dog_paths were created in the setup code")]),t._v("\nmy_model_accuracy "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" calc_accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hot_dog_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" not_hot_dog_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Fraction correct in small test set: {}"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("my_model_accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# checks that your function calc_accuracy works correctly")]),t._v("\nq_2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br")])]),a("p",[t._v("Solution:")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("calc_accuracy")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" paths_to_hotdog_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" paths_to_other_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# We'll use the counts for denominator of accuracy calculation")]),t._v("\n    num_hot_dog_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paths_to_hotdog_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    num_other_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("len")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paths_to_other_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    hotdog_image_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paths_to_hotdog_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    preds_for_hotdogs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("hotdog_image_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Summing list of binary variables gives a count of True values")]),t._v("\n    num_correct_hotdog_preds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds_for_hotdogs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    other_image_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" read_and_prep_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("paths_to_other_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    preds_other_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("other_image_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Number correct is the number judged not to be hot dogs")]),t._v("\n    num_correct_other_preds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_other_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("is_hot_dog"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preds_other_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    total_correct "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_correct_hotdog_preds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" num_correct_other_preds\n    total_preds "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" num_hot_dog_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" num_other_images\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" total_correct "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" total_preds\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br")])]),a("h4",{attrs:{id:"exercise-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-3"}},[t._v("#")]),t._v(" Exercise 3:")]),t._v(" "),a("p",[t._v("There are other models besides the ResNet model (which we have loaded). For example, an earlier winner of the ImageNet competition is the VGG16 model.  Don't worry about the differences between these models yet. We'll come back to that later. For now, just focus on the mechanics of applying these models to a problem.")]),t._v(" "),a("p",[t._v("The code used to load a pretrained ResNet50 model was")]),t._v(" "),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v("my_model = ResNet50(weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[t._v("The weights for the model are stored at "),a("code",[t._v("../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5")]),t._v(".")]),t._v(" "),a("p",[t._v("In the cell below, create a VGG16 model with the preloaded weights. Then use your "),a("code",[t._v("calc_accuracy")]),t._v(" function to determine what fraction of images the VGG16 model correctly classifies.  Is it better or worse than the pretrained ResNet model?")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# import the model")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" VGG16\n\n\nvgg16_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" VGG16"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# calculate accuracy on small dataset as a test")]),t._v("\nvgg16_accuracy "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" calc_accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vgg16_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" hot_dog_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" not_hot_dog_paths"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Fraction correct in small dataset: {}"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("format")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("vgg16_accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nq_3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br")])]),a("p",[t._v("If this model is used for an app that runs on a phone, what factors besides accuracy might you care about? After you've thought about it, keep going below.")]),t._v(" "),a("h2",{attrs:{id:"_4-transfer-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-transfer-learning"}},[t._v("#")]),t._v(" 4. Transfer Learning")]),t._v(" "),a("h3",{attrs:{id:"intro-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-6"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[a("strong",[t._v("This is Lesson 4 in the "),a("a",{attrs:{href:"https://www.kaggle.com/learn/deep-learning",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Learning"),a("OutboundLink")],1),t._v(" track")])]),t._v(" "),a("p",[t._v("At the end of this lesson, you will be able to use transfer learning to build highly accurate computer vision models for your custom purposes, even when you have relatively little data.")]),t._v(" "),a("h3",{attrs:{id:"lesson-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-4"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'mPFq5KMxKVw'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/mPFq5KMxKVw" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"sample-code-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-code-2"}},[t._v("#")]),t._v(" Sample Code")]),t._v(" "),a("h4",{attrs:{id:"specify-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#specify-model"}},[t._v("#")]),t._v(" Specify Model")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ResNet50\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" GlobalAveragePooling2D\n\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nresnet_weights_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'")]),t._v("\n\nmy_new_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("include_top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pooling"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'avg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("resnet_weights_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Say not to train first layer (ResNet) model. It is already trained")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainable "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br")])]),a("h4",{attrs:{id:"compile-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#compile-model"}},[t._v("#")]),t._v(" Compile Model")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("my_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h4",{attrs:{id:"fit-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fit-model"}},[t._v("#")]),t._v(" Fit Model")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet50 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" preprocess_input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ImageDataGenerator\n\nimage_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("\ndata_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataGenerator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preprocessing_function"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\ntrain_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/urban-and-rural-photos/rural_and_urban_photos/train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("24")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nvalidation_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/urban-and-rural-photos/rural_and_urban_photos/val'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        train_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        steps_per_epoch"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validation_data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("validation_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validation_steps"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br")])]),a("h4",{attrs:{id:"note-on-results"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#note-on-results"}},[t._v("#")]),t._v(" Note on Results:")]),t._v(" "),a("p",[t._v("The printed validation accuracy can be meaningfully better than the training accuracy at this stage. This can be puzzling at first.")]),t._v(" "),a("p",[t._v("It occurs because the training accuracy was calculated at multiple points as the network was improving (the numbers in the convolutions were being updated to make the model more accurate). The network was inaccurate when the model saw the first training images, since the weights hadn't been trained/improved much yet. Those first training results were averaged into the measure above.")]),t._v(" "),a("p",[t._v("The validation loss and accuracy measures were calculated "),a("strong",[t._v("after")]),t._v(" the model had gone through all the data. So the network had been fully trained when these scores were calculated.")]),t._v(" "),a("p",[t._v("This isn't a serious issue in practice, and we tend not to worry about it.")]),t._v(" "),a("h3",{attrs:{id:"exercise-transfer-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-transfer-learning"}},[t._v("#")]),t._v(" Exercise: Transfer Learning")]),t._v(" "),a("p",[a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/kernels/fork/532365",target:"_blank",rel:"noopener noreferrer"}},[t._v("Try transfer learning"),a("OutboundLink")],1)]),t._v(" yourself.")]),t._v(" "),a("h4",{attrs:{id:"exercise-introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-introduction"}},[t._v("#")]),t._v(" Exercise Introduction")]),t._v(" "),a("p",[t._v("The cameraman who shot our deep learning videos mentioned a problem that we can solve with deep learning.")]),t._v(" "),a("p",[t._v("He offers a service that scans photographs to store them digitally.  He uses a machine that quickly scans many photos. But depending on the orientation of the original photo, many images are digitized sideways.  He fixes these manually, looking at each photo to determine which ones to rotate.")]),t._v(" "),a("p",[t._v("In this exercise, you will build a model that distinguishes which photos are sideways and which are upright, so an app could automatically rotate each image if necessary.")]),t._v(" "),a("p",[t._v("If you were going to sell this service commercially, you might use a large dataset to train the model. But you'll have great success with even a small dataset.  You'll work with a small dataset of dog pictures, half of which are rotated sideways.")]),t._v(" "),a("p",[t._v("Specifying and compiling the model look the same as in the example you've seen. But you'll need to make some changes to fit the model.")]),t._v(" "),a("p",[a("strong",[t._v("Run the following cell to set up automatic feedback.")])]),t._v(" "),a("h4",{attrs:{id:"_1-specify-the-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-specify-the-model"}},[t._v("#")]),t._v(" 1) Specify the Model")]),t._v(" "),a("p",[t._v("Since this is your first time, we'll provide some starter code for you to modify. You will probably copy and modify code the first few times you work on your own projects.")]),t._v(" "),a("p",[t._v("There are some important parts left blank in the following code.")]),t._v(" "),a("p",[t._v("Fill in the blanks (marked with "),a("code",[t._v("____")]),t._v(") and run the cell")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ResNet50\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" GlobalAveragePooling2D\n\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nresnet_weights_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'")]),t._v("\n\nmy_new_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("include_top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pooling"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'avg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("resnet_weights_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Indicate whether the first layer should be trained/changed or not.")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainable "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n\nstep_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br")])]),a("h4",{attrs:{id:"_2-compile-the-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-compile-the-model"}},[t._v("#")]),t._v(" 2) Compile the Model")]),t._v(" "),a("p",[t._v("You now compile the model with the following line.  Run this cell.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("my_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                     loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" \n                     metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br")])]),a("p",[t._v("That ran nearly instantaneously.  Deep learning models have a reputation for being computationally demanding.  Why did that run so quickly?")]),t._v(" "),a("p",[t._v("After thinking about this, check your answer by uncommenting the cell below.")]),t._v(" "),a("p",[a("strong",[t._v("Solution")]),t._v(": The compile model doesn't change the values in any convolutions. In fact, your model has not even received an argument with data yet. Compile specifies how your model will make updates a later "),a("code",[t._v("fit")]),t._v(" step where it receives data. That is the part that will take longer.")]),t._v(" "),a("h4",{attrs:{id:"_3-review-the-compile-step"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-review-the-compile-step"}},[t._v("#")]),t._v(" 3) Review the Compile Step")]),t._v(" "),a("p",[t._v("You provided three arguments in the compile step.")]),t._v(" "),a("ul",[a("li",[t._v("optimizer")]),t._v(" "),a("li",[t._v("loss")]),t._v(" "),a("li",[t._v("metrics")])]),t._v(" "),a("p",[t._v("Which arguments could affect the accuracy of the predictions that come out of the model?  After you have your answer, run the cell below to see the solution.")]),t._v(" "),a("p",[a("strong",[t._v("Solution")]),t._v(":")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("optimizer")]),t._v(" determines how we determine the numerical values that make up the model. So it can affect the resulting model and predictions")]),t._v(" "),a("li",[a("strong",[t._v("loss")]),t._v(" determines what goal we optimize when determining numerical values in the model. So it can affect the resulting model and predictions")]),t._v(" "),a("li",[a("strong",[t._v("metrics")]),t._v(" determines only what we print out while the model is being built, but it doesn't affect the model itself.")])]),t._v(" "),a("p",[t._v("You may not understand all of this yet. That's totally fine for now. It will become clearer in an upcoming lesson (called A Deeper Understanding of Deep Learning).")]),t._v(" "),a("h4",{attrs:{id:"_4-fit-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-fit-model"}},[t._v("#")]),t._v(" 4) Fit Model")]),t._v(" "),a("p",[a("strong",[t._v("Your training data is in the directory "),a("code",[t._v("../input/dogs-gone-sideways/images/train")]),t._v(". The validation data is in "),a("code",[t._v("../input/dogs-gone-sideways/images/val")])]),t._v(". Use that information when setting up "),a("code",[t._v("train_generator")]),t._v(" and "),a("code",[t._v("validation_generator")]),t._v(".")]),t._v(" "),a("p",[t._v("You have 220 images of training data and 217 of validation data.  For the training generator, we set a batch size of 10. Figure out the appropriate value of "),a("code",[t._v("steps_per_epoch")]),t._v(" in your "),a("code",[t._v("fit_generator")]),t._v(" call.")]),t._v(" "),a("p",[t._v("Fill in all the blanks (again marked as "),a("code",[t._v("____")]),t._v(").  Then run the cell of code.  Watch as your model trains the weights and the accuracy improves.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet50 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" preprocess_input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ImageDataGenerator\n\nimage_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("\ndata_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataGenerator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                                        directory"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/dogs-gone-sideways/images/train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                        batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nvalidation_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n                                        directory"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/dogs-gone-sideways/images/val'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# fit_stats below saves some statistics describing how model fitting went")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# the key role of the following line is how it changes my_new_model by fitting to data")]),t._v("\nfit_stats "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" my_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                       steps_per_epoch"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("22")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                       validation_data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("validation_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                       validation_steps"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nstep_4"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br")])]),a("p",[t._v("Can you tell from the results what fraction of the time your model was correct in the validation data?")]),t._v(" "),a("p",[t._v("In the next step, we'll see if we can improve on that.")]),t._v(" "),a("h2",{attrs:{id:"_5-data-augmentation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-data-augmentation"}},[t._v("#")]),t._v(" 5. Data Augmentation")]),t._v(" "),a("h3",{attrs:{id:"intro-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-7"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("At the end of this lesson, you will be able to use data augmentation. This trick that makes it seem like you have far more data than you actually have, resulting in even better models..")]),t._v(" "),a("h3",{attrs:{id:"lesson-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-5"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'ypt_BAotCLo'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/ypt_BAotCLo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"sample-code-3"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-code-3"}},[t._v("#")]),t._v(" Sample Code")]),t._v(" "),a("p",[t._v("We have some model set-up code which you've seen before. It's not our focus for the moment, so it is hidden (but optionally visible by clicking the \"code\" button below.)")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ResNet50\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" GlobalAveragePooling2D\n\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nresnet_weights_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'")]),t._v("\n\nmy_new_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("include_top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pooling"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'avg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("resnet_weights_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Say not to train first layer (ResNet) model. It is already trained")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainable "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br")])]),a("h4",{attrs:{id:"fitting-a-model-with-data-augmentation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#fitting-a-model-with-data-augmentation"}},[t._v("#")]),t._v(" Fitting a Model With Data Augmentation")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("!ls "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("input")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("urban"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("and")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("rural"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v("photos"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v("rural_and_urban_photos\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet50 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" preprocess_input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ImageDataGenerator\n\nimage_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("\n\ndata_generator_with_aug "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataGenerator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preprocessing_function"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   horizontal_flip"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   width_shift_range "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                   height_shift_range "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ntrain_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator_with_aug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/urban-and-rural-photos/rural_and_urban_photos/train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("24")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ndata_generator_no_aug "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataGenerator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preprocessing_function"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nvalidation_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator_no_aug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/urban-and-rural-photos/rural_and_urban_photos/val'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        train_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        steps_per_epoch"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validation_data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("validation_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validation_steps"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br")])]),a("h3",{attrs:{id:"exercise-data-augmentation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-data-augmentation"}},[t._v("#")]),t._v(" Exercise: Data Augmentation")]),t._v(" "),a("p",[t._v("Move on to "),a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/kernels/fork/536195",target:"_blank",rel:"noopener noreferrer"}},[t._v("apply data augmentation"),a("OutboundLink")],1)]),t._v(" yourself.")]),t._v(" "),a("h4",{attrs:{id:"exercise-introduction-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-introduction-2"}},[t._v("#")]),t._v(" Exercise Introduction")]),t._v(" "),a("p",[t._v("We will return to the automatic rotation problem you worked on in the previous exercise. But we'll add data augmentation to improve your model.")]),t._v(" "),a("p",[t._v("The model specification and compilation steps don't change when you start using data augmentation. The code you've already worked with for specifying and compiling a model is in the cell below.  Run it so you'll be ready to work on data augmentation.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ResNet50\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" GlobalAveragePooling2D\n\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),t._v("\nresnet_weights_path "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'")]),t._v("\n\nmy_new_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("ResNet50"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("include_top"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" pooling"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'avg'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" weights"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("resnet_weights_path"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("trainable "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("False")]),t._v("\n\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'sgd'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Set up code checking")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" binder\nbinder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bind"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("globals")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deep_learning"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exercise_5 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Setup Complete"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br")])]),a("h4",{attrs:{id:"_1-fit-the-model-using-data-augmentation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-fit-the-model-using-data-augmentation"}},[t._v("#")]),t._v(" 1) Fit the Model Using Data Augmentation")]),t._v(" "),a("p",[t._v("Here is some code to set up some ImageDataGenerators. Run it, and then answer the questions below about it.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("applications"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("resnet50 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" preprocess_input\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("preprocessing"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("image "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" ImageDataGenerator\n\nimage_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("224")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Specify the values for all arguments to data_generator_with_aug.")]),t._v("\ndata_generator_with_aug "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataGenerator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preprocessing_function"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                              horizontal_flip "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                              width_shift_range "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                                              height_shift_range "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n            \ndata_generator_no_aug "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" ImageDataGenerator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("preprocessing_function"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("preprocess_input"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br")])]),a("p",[t._v("Why do we need both a generator with augmentation and a generator without augmentation? After thinking about it, check out the solution below.")]),t._v(" "),a("p",[a("strong",[t._v("Solution")]),t._v(": We want to do data augmentation when fitting the model for the reasons mentioned in the video (including a reduction in overfitting, by giving us more data to work with).")]),t._v(" "),a("p",[t._v("But we don't want to change how we test the model. So the validation generator will use an ImageDataGenerator without augmentation. That allows a straightforward comparison between different training procedures (e.g. training with augmentation and without it).")]),t._v(" "),a("p",[t._v("If the augmentation made it harder to predict the label associated with an image (e.g. because of how the image was cropped in augmentation) that would make it misleading to compare scores to another procedure where the validation data was only original images.")]),t._v(" "),a("h4",{attrs:{id:"_2-choosing-augmentation-types"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-choosing-augmentation-types"}},[t._v("#")]),t._v(" 2) Choosing Augmentation Types")]),t._v(" "),a("p",[t._v("ImageDataGenerator offers many types of data augmentation. For example, one argument is "),a("code",[t._v("rotation_range")]),t._v(". This rotates each image by a random amount that can be up to whatever value you specify.")]),t._v(" "),a("p",[t._v("Would it be sensible to use automatic rotation for this problem?  Why or why not?")]),t._v(" "),a("p",[a("strong",[t._v("Solution")]),t._v(": The goal in this problem is to tell if an image is upright or sideways. Rotating images moderately might cause images that don't feel cleanly in either category.")]),t._v(" "),a("p",[t._v("Since data augmentation affects images without touching the labels, a dramatic rotation would mean some images are used for training with the wrong label (e.g. an image would be rotated sideways by the generator, and still have a label of being upright)")]),t._v(" "),a("h4",{attrs:{id:"_3-code"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-code"}},[t._v("#")]),t._v(" 3) Code")]),t._v(" "),a("p",[t._v("Fill in the missing pieces in the following code. We've supplied some boilerplate. You need to think about what ImageDataGenerator is used for each data source.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Specify which type of ImageDataGenerator above is to load in training data")]),t._v("\ntrain_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator_with_aug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        directory "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/dogs-gone-sideways/images/train'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Specify which type of ImageDataGenerator above is to load in validation data")]),t._v("\nvalidation_generator "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_generator_no_aug"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("flow_from_directory"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        directory "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'../input/dogs-gone-sideways/images/val'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        target_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" image_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        class_mode"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmy_new_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n        train_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# if you don't know what argument goes first, try the hint")]),t._v("\n        epochs "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        steps_per_epoch"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("19")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n        validation_data"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("validation_generator"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nq_3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br")])]),a("h4",{attrs:{id:"_4-did-data-augmentation-help"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-did-data-augmentation-help"}},[t._v("#")]),t._v(" 4) Did Data Augmentation Help?")]),t._v(" "),a("p",[t._v("How could you test whether data augmentation improved your model accuracy?")]),t._v(" "),a("p",[a("strong",[t._v("Solution")]),t._v(": Create "),a("code",[t._v("train_generator")]),t._v(" usng "),a("code",[t._v("data_generator_no_aug")]),t._v(" but don't change other arguments to "),a("code",[t._v("train_generator")]),t._v(".")]),t._v(" "),a("p",[t._v("Run the model and see the resuling accuracy. Compare this to the accuracy you got when "),a("code",[t._v("train_generator")]),t._v(" used augmentation.")]),t._v(" "),a("p",[t._v("Our validation dataset is very small, so there's a little bit of luck or randomness in the exact score from any model run. Validation scores will be more reliable as you start using larger datasets.")]),t._v(" "),a("h2",{attrs:{id:"_6-a-deeper-understanding-of-deep-learning"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-a-deeper-understanding-of-deep-learning"}},[t._v("#")]),t._v(" 6. A Deeper Understanding of Deep Learning")]),t._v(" "),a("h3",{attrs:{id:"intro-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-8"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[t._v("At the end of this lesson, you will understand how stochastic gradient descent and back-propagation are used to set the weights in a deep learning model. These topics are complex, but many experts view them as the most important ideas in deep learning.")]),t._v(" "),a("h3",{attrs:{id:"lesson-6"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-6"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'kQmHaI5Jw1c'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/kQmHaI5Jw1c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("p",[a("strong",[t._v("Links Mentioned")])]),t._v(" "),a("p",[a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning",target:"_blank",rel:"noopener noreferrer"}},[t._v("ReLU activation function"),a("OutboundLink")],1)])]),t._v(" "),a("h2",{attrs:{id:"_7-deep-learning-from-scratch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_7-deep-learning-from-scratch"}},[t._v("#")]),t._v(" 7. Deep Learning From Scratch")]),t._v(" "),a("h3",{attrs:{id:"intro-9"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-9"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[a("strong",[t._v("This is Lesson 7 in the "),a("a",{attrs:{href:"https://www.kaggle.com/learn/deep-learning",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Learning"),a("OutboundLink")],1),t._v(" course")])]),t._v(" "),a("p",[t._v("The models you've built so far have relied on pre-trained models. But they aren't the ideal solution for many use cases. In this lesson, you will learn how to build totally new models.")]),t._v(" "),a("h3",{attrs:{id:"lesson-7"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-7"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'YbNE3zhtsoo'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/YbNE3zhtsoo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"sample-code-4"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-code-4"}},[t._v("#")]),t._v(" Sample Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" keras\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dropout\n\n\nimg_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v("\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("data_prep")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    out_y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_categorical"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    num_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    x_as_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    x_shaped_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_as_array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_shaped_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_y\n\ntrain_file "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"../input/digit-recognizer/train.csv"')]),t._v("\nraw_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_prep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 input_shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          validation_split "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br"),a("span",{staticClass:"line-number"},[t._v("35")]),a("br"),a("span",{staticClass:"line-number"},[t._v("36")]),a("br"),a("span",{staticClass:"line-number"},[t._v("37")]),a("br"),a("span",{staticClass:"line-number"},[t._v("38")]),a("br"),a("span",{staticClass:"line-number"},[t._v("39")]),a("br"),a("span",{staticClass:"line-number"},[t._v("40")]),a("br"),a("span",{staticClass:"line-number"},[t._v("41")]),a("br")])]),a("h3",{attrs:{id:"exercise-deep-learning-from-scratch"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-deep-learning-from-scratch"}},[t._v("#")]),t._v(" Exercise: Deep Learning From Scratch")]),t._v(" "),a("p",[t._v("You are ready to "),a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/kernels/fork/574269",target:"_blank",rel:"noopener noreferrer"}},[t._v("build your own model"),a("OutboundLink")],1)]),t._v(".")]),t._v(" "),a("h4",{attrs:{id:"introduction"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),a("p",[t._v("You've seen how to build a model from scratch to identify handwritten digits.  You'll now build a model to identify different types of clothing.  To make models that train quickly, we'll work with very small (low-resolution) images.")]),t._v(" "),a("p",[t._v("As an example, your model will take an images like this and identify it as a shoe:")]),t._v(" "),a("center",[a("img",{attrs:{src:"https://i.imgur.com/GyXOnSB.png",width:"20%",height:"20%",alt:"shoes"}})]),t._v(" "),a("h4",{attrs:{id:"data-preparation"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#data-preparation"}},[t._v("#")]),t._v(" Data Preparation")]),t._v(" "),a("p",[t._v("This code is supplied, and you don't need to change it. Just run the cell below.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" keras\n\nimg_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v("\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("prep_data")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    out_y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_categorical"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    num_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_y\n\nfashion_file "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"../input/fashionmnist/fashion-mnist_train.csv"')]),t._v("\nfashion_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loadtxt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fashion_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skiprows"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" delimiter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prep_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fashion_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Set up code checking")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" binder\nbinder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bind"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("globals")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deep_learning"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exercise_7 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Setup Complete"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br")])]),a("h4",{attrs:{id:"_1-start-the-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-start-the-model"}},[t._v("#")]),t._v(" 1) Start the model")]),t._v(" "),a("p",[t._v("Create a "),a("code",[t._v("Sequential")]),t._v(" model called "),a("code",[t._v("fashion_model")]),t._v(". Don't add layers yet.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" keras\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Conv2D\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Your Code Here")]),t._v("\nfashion_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nq_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("h4",{attrs:{id:"_2-add-the-first-layer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-add-the-first-layer"}},[t._v("#")]),t._v(" 2) Add the first layer")]),t._v(" "),a("p",[t._v("Add the first "),a("code",[t._v("Conv2D")]),t._v(" layer to "),a("code",[t._v("fashion_model")]),t._v(". It should have 12 filters, a kernel_size of 3 and the "),a("code",[t._v("relu")]),t._v(" activation function. The first layer always requires that you specify the "),a("code",[t._v("input_shape")]),t._v(".  We have saved the number of rows and columns to the variables "),a("code",[t._v("img_rows")]),t._v(" and "),a("code",[t._v("img_cols")]),t._v(" respectively, so the input shape in this case is "),a("code",[t._v("(img_rows, img_cols, 1)")]),t._v(".")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Your code here")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 input_shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nq_2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h4",{attrs:{id:"_3-add-the-remaining-layers"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-add-the-remaining-layers"}},[t._v("#")]),t._v(" 3) Add the remaining layers")]),t._v(" "),a("ol",[a("li",[t._v("Add 2 more convolutional ("),a("code",[t._v("Conv2D layers")]),t._v(") with 20 filters each, 'relu' activation, and a kernel size of 3. Follow that with a "),a("code",[t._v("Flatten")]),t._v(" layer, and then a "),a("code",[t._v("Dense")]),t._v(" layer with 100 neurons.")]),t._v(" "),a("li",[t._v("Add your prediction layer to "),a("code",[t._v("fashion_model")]),t._v(".  This is a "),a("code",[t._v("Dense")]),t._v(" layer.  We alrady have a variable called "),a("code",[t._v("num_classes")]),t._v(".  Use this variable when specifying the number of nodes in this layer. The activation should be "),a("code",[t._v("softmax")]),t._v(" (or you will have problems later).")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Your code here")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nq_3"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("h4",{attrs:{id:"_4-compile-your-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_4-compile-your-model"}},[t._v("#")]),t._v(" 4) Compile Your Model")]),t._v(" "),a("p",[t._v("Compile fashion_model with the "),a("code",[t._v("compile")]),t._v(" method.  Specify the following arguments:")]),t._v(" "),a("ol",[a("li",[a("code",[t._v('loss = "categorical_crossentropy"')])]),t._v(" "),a("li",[a("code",[t._v("optimizer = 'adam'")])]),t._v(" "),a("li",[a("code",[t._v("metrics = ['accuracy']")])])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Your code to compile the model in this cell")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nq_4"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br")])]),a("h4",{attrs:{id:"_5-fit-the-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_5-fit-the-model"}},[t._v("#")]),t._v(" 5) Fit The Model")]),t._v(" "),a("p",[t._v("Run the command "),a("code",[t._v("fashion_model.fit")]),t._v(". The arguments you will use are")]),t._v(" "),a("ol",[a("li",[t._v("The data used to fit the model. First comes the data holding the images, and second is the data with the class labels to be predicted. Look at the first code cell (which was supplied to you) where we called "),a("code",[t._v("prep_data")]),t._v(" to find the variable names for these.")]),t._v(" "),a("li",[a("code",[t._v("batch_size = 100")])]),t._v(" "),a("li",[a("code",[t._v("epochs = 4")])]),t._v(" "),a("li",[a("code",[t._v("validation_split = 0.2")])])]),t._v(" "),a("p",[t._v("When you run this command, you can watch your model start improving.  You will see validation accuracies after each epoch.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Your code to fit the model here")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          validation_split "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nq_5"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("h4",{attrs:{id:"_6-create-a-new-model"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_6-create-a-new-model"}},[t._v("#")]),t._v(" 6) Create A New Model")]),t._v(" "),a("p",[t._v("Create a new model called "),a("code",[t._v("second_fashion_model")]),t._v(" in the cell below.  Make some changes so it is different than "),a("code",[t._v("fashion_model")]),t._v(" that you've trained above. The change could be using a different number of layers, different number of convolutions in the layers, etc.")]),t._v(" "),a("p",[t._v("Define the model, compile it and fit it in the cell below.  See how it's validation score compares to that of the original model.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Your code below")]),t._v("\nsecond_fashion_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                         input_shape "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Changed kernel sizes to be 2")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# added an addition Conv2D layer")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# It is important not to change the last layer. First argument matches number of classes. Softmax guarantees we get reasonable probabilities")]),t._v("\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'categorical_crossentropy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                             optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                             metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nsecond_fashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" validation_split"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nq_6"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br")])]),a("h2",{attrs:{id:"_8-dropout-and-strides-for-larger-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_8-dropout-and-strides-for-larger-models"}},[t._v("#")]),t._v(" 8. Dropout and Strides for Larger Models")]),t._v(" "),a("h3",{attrs:{id:"intro-10"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#intro-10"}},[t._v("#")]),t._v(" Intro")]),t._v(" "),a("p",[a("strong",[t._v("This is Lesson 8 in the "),a("a",{attrs:{href:"https://www.kaggle.com/learn/deep-learning",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Learning"),a("OutboundLink")],1),t._v(" track")])]),t._v(" "),a("p",[t._v("At the end of this lesson, you will understand and know how to use")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("Stride lengths")]),t._v(" to make your model faster and reduce memory consumption")]),t._v(" "),a("li",[a("strong",[t._v("Dropout")]),t._v(" to combat overfitting")])]),t._v(" "),a("p",[t._v("Both of these techniques are especially useful in large models.")]),t._v(" "),a("h3",{attrs:{id:"lesson-8"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#lesson-8"}},[t._v("#")]),t._v(" Lesson")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" IPython"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("display "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" YouTubeVideo\nYouTubeVideo"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fwNLf4t7MR8'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" width"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" height"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("320")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br")])]),a("div",{staticClass:"language- line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-text"}},[a("code",[t._v('<iframe width=100% height=320 src="https://www.youtube.com/embed/fwNLf4t7MR8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>\n')])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br")])]),a("h3",{attrs:{id:"sample-code-5"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#sample-code-5"}},[t._v("#")]),t._v(" Sample Code")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" pandas "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" pd\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" keras\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("python"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dropout\n\nimg_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v("\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("data_prep")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    out_y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_categorical"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n    num_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    x_as_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("values"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    x_shaped_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_as_array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x_shaped_array "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_y\n\ntrain_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30000")]),t._v("\ntrain_file "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"../input/digit-recognizer/train.csv"')]),t._v("\nraw_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" pd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read_csv"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("train_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" data_prep"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 strides"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 input_shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("30")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dropout"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.5")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmodel"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          validation_split "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br"),a("span",{staticClass:"line-number"},[t._v("27")]),a("br"),a("span",{staticClass:"line-number"},[t._v("28")]),a("br"),a("span",{staticClass:"line-number"},[t._v("29")]),a("br"),a("span",{staticClass:"line-number"},[t._v("30")]),a("br"),a("span",{staticClass:"line-number"},[t._v("31")]),a("br"),a("span",{staticClass:"line-number"},[t._v("32")]),a("br"),a("span",{staticClass:"line-number"},[t._v("33")]),a("br"),a("span",{staticClass:"line-number"},[t._v("34")]),a("br"),a("span",{staticClass:"line-number"},[t._v("35")]),a("br"),a("span",{staticClass:"line-number"},[t._v("36")]),a("br"),a("span",{staticClass:"line-number"},[t._v("37")]),a("br"),a("span",{staticClass:"line-number"},[t._v("38")]),a("br"),a("span",{staticClass:"line-number"},[t._v("39")]),a("br"),a("span",{staticClass:"line-number"},[t._v("40")]),a("br"),a("span",{staticClass:"line-number"},[t._v("41")]),a("br"),a("span",{staticClass:"line-number"},[t._v("42")]),a("br"),a("span",{staticClass:"line-number"},[t._v("43")]),a("br"),a("span",{staticClass:"line-number"},[t._v("44")]),a("br")])]),a("h3",{attrs:{id:"exercise-dropout-and-strides-for-larger-models"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#exercise-dropout-and-strides-for-larger-models"}},[t._v("#")]),t._v(" Exercise: Dropout and Strides for Larger Models")]),t._v(" "),a("p",[a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/kernels/fork/663261",target:"_blank",rel:"noopener noreferrer"}},[t._v("Apply dropout and strides"),a("OutboundLink")],1)]),t._v(" yourself while experimenting with larger models.")]),t._v(" "),a("h4",{attrs:{id:"introduction-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#introduction-2"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),a("p",[t._v("You've built a model to identify clothing types in the "),a("strong",[t._v("MNIST for Fashion")]),t._v(" dataset.  Now you will make your model bigger, specify larger stride lengths and apply dropout. These changes will make your model faster and more accurate.")]),t._v(" "),a("p",[t._v("This is a last step in the "),a("strong",[a("a",{attrs:{href:"https://www.kaggle.com/learn/deep-learning",target:"_blank",rel:"noopener noreferrer"}},[t._v("Deep Learning Track"),a("OutboundLink")],1)]),t._v(".")]),t._v(" "),a("h4",{attrs:{id:"data-preparation-2"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#data-preparation-2"}},[t._v("#")]),t._v(" Data Preparation")]),t._v(" "),a("p",[a("strong",[t._v("Run this cell of code.")])]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("as")]),t._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" sklearn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("model_selection "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" train_test_split\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" keras\n\n"),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Set up code checking")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("core "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" binder\nbinder"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("bind"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("globals")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" learntools"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("deep_learning"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("exercise_8 "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"Setup Complete"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nimg_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("28")]),t._v("\nnum_classes "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[t._v("prep_data")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    out_y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("utils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("to_categorical"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    \n    x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    num_images "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" raw"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n    out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("reshape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_images"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" out_x "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("/")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("255")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" out_x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" out_y\n\nfashion_file "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"../input/fashionmnist/fashion-mnist_train.csv"')]),t._v("\nfashion_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("loadtxt"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fashion_file"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" skiprows"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" delimiter"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("','")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" prep_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fashion_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br"),a("span",{staticClass:"line-number"},[t._v("24")]),a("br"),a("span",{staticClass:"line-number"},[t._v("25")]),a("br"),a("span",{staticClass:"line-number"},[t._v("26")]),a("br")])]),a("h4",{attrs:{id:"_1-increasing-stride-size-in-a-layer"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-increasing-stride-size-in-a-layer"}},[t._v("#")]),t._v(" 1) Increasing Stride Size in A Layer")]),t._v(" "),a("p",[t._v("Below is a model without strides (or more accurately, with a stride length of 1)")]),t._v(" "),a("p",[t._v("Run it. Notice it's accuracy and how long it takes per epoch. Then you will change the stride length in one of the layers.")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("models "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Sequential\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("from")]),t._v(" tensorflow"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("layers "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Dropout\n\nbatch_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v("\n\nfashion_model "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 input_shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfashion_model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          validation_split "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br")])]),a("p",[t._v("You have the same code in the cell below, but the model is now called "),a("code",[t._v("fashion_model_1")]),t._v(".  Change the specification of "),a("code",[t._v("fashion_model_1")]),t._v(" so the second convolutional layer has a stride length of 2.")]),t._v(" "),a("p",[t._v("Run the cell after you have done that. How does the speed and accuracy change compared to the first model you ran above?")]),t._v(" "),a("div",{staticClass:"language-python line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[t._v("fashion_model_1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Sequential"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kernel_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n                 input_shape"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("img_rows"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" img_cols"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Conv2D"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" strides"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("128")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'relu'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("add"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("num_classes"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'softmax'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("compile")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("loss"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              optimizer"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'adam'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n              metrics"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v("'accuracy'")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nfashion_model_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("fit"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          batch_size"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          epochs"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("3")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n          validation_split "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nq_1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("check"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br")])]),a("p",[t._v("You should notice that your model training ran about twice as fast, but the accuracy change was trivial.")]),t._v(" "),a("p",[t._v("In addition to being faster to train, this model is also faster at making predictions. This is very important in many scenarios. In practice, you'll need to decide whether that type of speed is important in the applications where you eventually apply deep learning models.")]),t._v(" "),a("p",[t._v("You could experiment with more layers or more convolutions in each layer. With some fine-tuning, you can build a model that is both faster and more accurate than the original model.")]),t._v(" "),a("h4",{attrs:{id:"congrats"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#congrats"}},[t._v("#")]),t._v(" Congrats")]),t._v(" "),a("p",[t._v("You've finished the Deep Learning course.  You have the tools to create and tune computer vision models.")]),t._v(" "),a("p",[t._v("If you feel like playing more with this dataset, you can open up a new code cell to experiment with different models (adding dropout, adding layers, etc.)  Or pick a new project and try out your skills.")]),t._v(" "),a("p",[t._v("A few fun datasets you might try include:")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://www.kaggle.com/olgabelitskaya/classification-of-handwritten-letters",target:"_blank",rel:"noopener noreferrer"}},[t._v("Written letter recognition"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.kaggle.com/alxmamaev/flowers-recognition",target:"_blank",rel:"noopener noreferrer"}},[t._v("Flower Identification"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition",target:"_blank",rel:"noopener noreferrer"}},[t._v("Cats vs Dogs"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.kaggle.com/slothkong/10-monkey-species",target:"_blank",rel:"noopener noreferrer"}},[t._v("10 Monkeys"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"https://www.kaggle.com/kmader/rsna-bone-age",target:"_blank",rel:"noopener noreferrer"}},[t._v("Predict Bone Age from X-Rays"),a("OutboundLink")],1)])]),t._v(" "),a("p",[t._v("You have learned a lot, and you'll learn it as you practice. Have fun with it!")])],1)}),[],!1,null,null,null);s.default=e.exports}}]);